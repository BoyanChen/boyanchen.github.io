<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Final Project</title>
    <!--<link href="https://fonts.googleapis.com/css?family=Monoton" rel="stylesheet">-->
    <!--<link href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" rel="stylesheet">-->
    <!--<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,600,600i,700" rel="stylesheet">-->
    <link rel="stylesheet" href="https://use.typekit.net/cad1jof.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
<!--<ul>-->
<!--<li><img src="assets/logo.png"></li>-->
<!--<li class="navBtn"><a href="#">Resume</a></li>-->
<!--<li class="navBtn"><a href="about.html">About</a></li>-->
<!--<li class="navBtn"><a class="active" href="index.html">"Projects"</a></li>-->
<!--</ul>-->

<div class="navBar">
    <div class="logoContainer">
        <img src="assets/logo.png" width="83%" height="auto">
    </div>
    <div class="btnContainer">
        <div class="navBtn"><a href="#">Resume</a></div>
        <div class="navBtn"><a href="about.html">About</a></div>
        <div class="navBtn"><a href="index.html">"Projects"</a></div>
    </div>
</div>

<div class="projectIntro" style="background: #ffffff;width: 80%;margin-left: 10%; margin-top: 125px">
    <div class="projectText">
        <h1>Final Project</h1>
        <h3>A research project on the change in affordance and conceptual model by voice interaction</h3>
        <h3>A speculative contextual VUI for voice interaction in the near future</h3>
        <h3>Speculative project/Voice Interface/Future User Interface/Contextual User Interface</h3>
        <h3>04/05/2019</h3>
        <h3>In Progress</h3>
    </div>
</div>

<div class="container" id="bitGearPage">

    <div class="intro" style="margin-bottom: 100px">
        <h1>Intro</h1>
        <p>
           Nowadays, the voice user interface has become more and more accessible. Several technology giants like Apple, Google and Amazon already embeded the voice assistant in their
            product with a rational price. With the development of machine learning and Neural circuit in the chip. Voice recognition can be more precious, intuitive and closer to be able to perform
            human to human conversation. In this way, voice interaction has great potential to become a new dominant way of human computer interaction. This project is trying to looking for the
            problem which is going to happen on the UI/UX design for the voice interaction and provide solution.
        </p>
        <h1>Problem Identified</h1>
        <p style="font-size: 25px;font-style: italic;">
            "In voice user interfaces, you <span>cannot create visualÂ affordances</span>. Consequently, looking at one, users will have <span>no clear indications of what the interface can do</span> or <span>what their options are</span>.
            At the same time, users are <span>unsure of what they can expect from voice interaction</span>, because we normally associate voice with communication with other people rather than with
            technology."
        </p>
        <br>
        <p style="font-size: 20px;font-style: italic;float:right;">-Ditte Mortensen, How to Design Voice User Interface</p>
        <br>
        <br>
        <br>
        <p style="font-size: 25px;font-style: italic;">
            "An Explanation created by users, usually highly simplified, of how something works"
        </p>
        <br>
        <p style="font-size: 20px;font-style: italic;float:right;">-Don Norman Design of Everday Things</p>
        <br>
        <br>
        <br>
        <p>
            The traditional conceptual model is based on the click interaction of mouse. Or then "Enter" key on our keyboard. Both click and "Enter" is trying to form a conceptual model as a nested
            file structure as shown in the picture. In order to find the major studio file, I need to enter the current project file first. So that the conceptual model for the file system
            is like a nested branch, in computer's terminal, this is described as path. However, if we use voice interaction, we can go directly in to that folder just by saying its' name.
            So the conceptual model for voice interaction is supposed to be different. And if we can go to the folder or the file we want how could make the users know where am I right now? This is
            one of the problem that need to be solved in voice user experience building process.
        </p>
        <div class="imgDescription">
            <img src="assets/IndexAssetsOutput/MS2%20Final/Conceptual%20Model.png" width="100%" height="auto">
            <div class="description">
                <p>Path:/Users/Ronnie/Desktop/Current\ Project/19SP_MS2</p>
            </div>
        </div>

        <h1>Research Question</h1>
        <p style="font-size: 25px;">
            How voice as a mean of interaction will change the design of VUI in the perspective of typography and information architect, and how these changes will affect users' affordance and conceptual
            model for the conversational user experience of VUI
        </p>

        <div class="text-spliter" id="question">
            <div class="text-entry"><h3 class ="dynamicQuery defaultQuery" id="1">Prototype#1</h3></div>
            <div class="text-entry"><h3 class ="dynamicQuery" id="2">Prototype#2</h3></div>
            <div class="text-entry"><h3 class ="dynamicQuery" id="3">Prototype#3</h3></div>
        </div>

        <div class="dynamicContent defaultContent" id="content1">
        <h1>Prototype #1: Paper Prototype</h1>
        <p>
            This is the very first stage of the project, and the main goal for this paper prototype is testing the conceptual model I just came up as an idea for voice interaction. I decide
            to prioritize the conceptual model than affordance is because that I think the conceptual model is like a blueprint for the user experience. It conveys how you are going to define
            the expectation of using the interface for the users. Once you make sure this model is working for massive users. Than you can consider what kind of object for this blueprint that is
            helpful to build this conceptual model be built in users' mind.
        </p>
        <p>In order to narrow down the scope of research. I decide to narrow down to a specific context to use the voice interaction. And this context should be the frequently used one
        so that the tester are familiar with it. And I think it will be easier for them to talk about the difference. Finally I choose the web browser. Since it is the one of the most frequently
            used application in our daily life. Here are my choices and finally I decide to choose the web browsing experience.
        </p>
        <ul style="font-size: 20px">
            <li style="font-weight: bold">Web  Browser</li>
            <li>Text Editor like Microsoft Word, Pages</li>
            <li>eBook Reader</li>
        </ul>

        <div class="imgDescription" style="text-align: center">
            <img src="assets/IndexAssetsOutput/MS2%20Final/paperPrototyepOV.JPG" width="80%" height="auto">
            <div class="description">
                <p>Paper prototype</p>
            </div>
        </div>

        <h3>Conceptual Model Idea: Computer/Device as a servent</h3>
        <p>
            The conceptual model I came up with the voice user experience is computer as a servant which serve the content directly to the users. Previously the conceptual model for
            the users of interface is a "Desktop", a bunch of folders which contains the "content" we want to view. And there is no "servant" there to grab the content we want to use or see for
            the users need to look for them by themselves. To build this conceptual model, we have button which let us open something, menu to let us know what we can do
            , and pages or tabs to let us switch between different task. All of these purpose is optimizing our experience of searching.
            However, when the users start to use voice to communicate with the computer, we don't need to search by ourselves any more. Our voice is sending a command line for the computer
            and then let the computer execute our command for as. So that the users should be able to focus more on the content.
        </p>

        <div class="imgDescription" style="text-align: center">
            <img src="assets/IndexAssetsOutput/MS2%20Final/Home.JPG" width="80%" height="auto">
            <div class="description">
                <p>Home page</p>
            </div>
        </div>
        <div class="imgDescription" style="text-align: center">
            <img src="assets/IndexAssetsOutput/MS2%20Final/Result.JPG" width="80%" height="auto">
            <div class="description">
                <p>The result page of searching</p>
            </div>
        </div>
        <div class="imgDescription" style="text-align: center">
            <img src="assets/IndexAssetsOutput/MS2%20Final/Loading.JPG" width="80%" height="auto">
            <div class="description">
                <p>Instead of changing in page, it is changing elements on the table. Just like servant is wiping out the empty beer bottle</p>
            </div>
        </div>
        <div class="imgDescription" style="text-align: center">
            <img src="assets/IndexAssetsOutput/MS2%20Final/SwichPage.JPG" width="80%" height="auto">
            <div class="description">
                <p>The page just tell you what you previously seen and what sites had been shown</p>
            </div>
        </div>
            <h3>Feedback</h3>
            <ul style="font-size: 20px">
                <li>Looking for free conversation, the suggest command line might give users a feeling of being limited in options</li>
                <li>The difference of conceptual model is not noticeable</li>
                <li>The whole experience of the prototype feels buggy, hard to grasp the whole feeling of the prototype</li>
                <li>The suggest command line with underline still offers the affordance of clicking</li>
                <li>Put the tester into a certain context, such as ordering a food, or purchase something will be helpful for tester to give more feedback from experience wise</li>
            </ul>
        </div>
        <div class="dynamicContent" id="content2">
            <h1>Prototype #2: Interactive Demo Prototype</h1>
            <p>
                This is prototype is considered as an improved version for prototype#1 and continue test for the "voice assistance as a servent" idea in the prototype#1. Based on the feedback
                from the prototype#1, I made a interactive demo which is able to show the transition animation while users browse through different content on the voice browser. I also add a
                specific context of "searching upcoming event in NYC" as the goal of the whole prototype experience.
            </p>


            <div class="imgDescription" style="text-align: center">
                <img src="assets/IndexAssetsOutput/MS2%20Final/Home.svg" width="45%" height="auto">
                <img src="assets/IndexAssetsOutput/MS2%20Final/result.svg" width="45%" height="auto">
                <div class="description">
                    <p>Intro and result page</p>
                </div>
            </div>

            <h3>Quick search assistant</h3>
            <p>
                In this prototype, I also test a more specific function by using voice command to search the content from the content directly. The users can search where is the location of
                NYC international auto show by asking. Than the google map will be trigger automatically to tell the information for the users. I think it will make a less disruptive experience
                for searching, since they don't have to switch on another page for finishing this kind of quick search task.
            </p>

            <div class="imgDescription" style="text-align: center">
                <img src="assets/IndexAssetsOutput/MS2%20Final/autoshow%20site.svg" width="45%" height="auto">
                <img src="assets/IndexAssetsOutput/MS2%20Final/map.svg" width="45%" height="auto">
                <div class="description">
                    <p>The search information pop up at the left</p>
                </div>
            </div>

            <h3>Feedback</h3>
            <ul style="font-size: 20px">
                <li>Instead of just focus on create visual affordance, also consider using the audio or sound effect to create certain affordance for the users</li>
                <li>The web page lack of information to let the users know what could be done and what could be expected to see after the voice command</li>
                <li>Let the voice assistant read or speak the detail information of the autoshow feels lengthy and has the potential to make the users to feel impatient. Should make the users
                stop the introduction</li>
            </ul>

        </div>
        <div class="dynamicContent" id="content3">
            <h1>Prototype#3: In Progress...</h1>
        </div>

    </div>



</div>

<div class="footer">Boyan Chen Copyright Â© 2018</div>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="jquery.drawsvg.js"></script>
<script src="js.js"></script>
</body>
</html>